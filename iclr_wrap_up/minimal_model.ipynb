{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing activation functions for a minimal model\n",
    "The opposing paper argues that input compression is merely an artifact of double saturated activation functions. We test this assumption for the minimal model in a numeric simulation. We look at the development of mututal information with the input in a one neuron model with growing weights and compare different activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.arange(0.1, 8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input is sampled from a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_distribution = stats.norm()\n",
    "input_ = input_distribution.rvs(1000)\n",
    "plt.hist(input_, bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input is multiplied by the different weights. This is the pass from the input neuron to the hidden neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_input = np.outer(weights, input_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation functions we want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_sigmoid(x):\n",
    "    lower_bound = -2.5\n",
    "    upper_bound = 2.5\n",
    "    linear = 0.2 * x + 0.5\n",
    "    linear[x < lower_bound] = 0\n",
    "    linear[x > upper_bound] = 1\n",
    "    return linear\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "activation_functions = [tf.nn.sigmoid, tf.nn.tanh, tf.nn.relu, tf.nn.softsign, tf.nn.softplus, hard_sigmoid, \n",
    "                       tf.nn.selu, tf.nn.relu6, tf.nn.elu, tf.nn.leaky_relu, linear]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we look at the shape of the different activation functions. We see that some are double saturated like `tanh` and some are not like `relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "for actvation_function in activation_functions:\n",
    "    x = np.linspace(-5,5,100)\n",
    "    ax.plot(x, actvation_function(x), label=actvation_function.__name__)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=int(len(activation_functions)/3)+1, ncols=3, figsize=(10, 10))\n",
    "axes = axes.flat\n",
    "\n",
    "for i, actvation_function in enumerate(activation_functions):\n",
    "    x = np.linspace(-10,10,100)\n",
    "    axes[i].plot(x, actvation_function(x))\n",
    "    axes[i].set(title=actvation_function.__name__)\n",
    "    \n",
    "# Remove unused plots.\n",
    "for ax in axes:\n",
    "    if not ax.lines:\n",
    "        ax.axis('off')\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the activation functions to the weighted inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "for actvation_function in activation_functions:\n",
    "    try:\n",
    "        outputs[actvation_function.__name__] = actvation_function(net_input).numpy()\n",
    "    except AttributeError:\n",
    "        outputs[actvation_function.__name__] = actvation_function(net_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now estimate the discrete mututal information between the input $X$ and the activity of the hidden neuron $Y$, which is in this case also the output. $H(Y|X) = 0$, since $Y$ is a deterministic function of $X$. Therefore\n",
    "\n",
    "\\begin{align}\n",
    "I(X;Y) &= H(Y) - H(Y|X)\\\\\n",
    "       &= H(Y)\\\\\n",
    "\\end{align}\n",
    "\n",
    "The entropy of the input is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dig, _ = np.histogram(input_, 50)\n",
    "print(f'{stats.entropy(dig, base=2):.2f} bits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper a fixed number of bins is evenly distributed between the minimum and the maximum activation over all weight values. The result below is indeed comparable to the paper and shows that mutual information decreases only in double saturated activation functions, while it increases otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=len(outputs), figsize=(10, 20), sharey=True)\n",
    "ax = ax.flat\n",
    "for ax_idx, (activation_function, Y) in enumerate(outputs.items()):\n",
    "    min_acitivity = Y.min()\n",
    "    max_acitivity = Y.max()\n",
    "    \n",
    "    mi = np.zeros(len(weights))\n",
    "    for i in range(len(weights)):\n",
    "        bins = np.linspace(min_acitivity, max_acitivity, 50)\n",
    "        digitized, _ = np.histogram(Y[i], bins=bins)\n",
    "        \n",
    "        mi[i] = stats.entropy(digitized, base=2) \n",
    "\n",
    "    ax[ax_idx].plot(weights,  mi)\n",
    "    ax[ax_idx].set(title=f'{activation_function}; max H = {mi.max():.2f}', xlabel='w', ylabel='I(X;T)') \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet the fact that mutual information increases even in the linear case is a result of binning between the **minimum and the maximum activation of all neurons**. It raises the question whether this approch is sensible at all or whether binning boundaries should be determined for each simulated weight value seperately. We compare the approach with creating a fixed number of bins between the **minimum and the maximum activity for each weight**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=len(outputs), figsize=(10, 20), sharey=True)\n",
    "ax = ax.flat\n",
    "for ax_idx, (activation_function, Y) in enumerate(outputs.items()):\n",
    "    \n",
    "    mi = np.zeros(len(weights))\n",
    "    for i in range(len(weights)):\n",
    "        digitized, _ = np.histogram(Y[i], bins=50)\n",
    "        mi[i] = stats.entropy(digitized, base=2) \n",
    "\n",
    "    ax[ax_idx].plot(weights,  mi)\n",
    "    ax[ax_idx].set(title=f'{activation_function}; max H = {mi.max():.2f}', xlabel='w', ylabel='I(X;T)', ylim=[0,7]) \n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a more sensible result. The mutual information is now constant in the linear case and has the same value as entropy of the input. We now see that mutual information stays constant for many non saturated activation functions, while it still decreases for double saturated functions. Yet it also decreases for some non-double saturated functions such as `elu` and `softplus`. \n",
    "\n",
    "Moreover, we see that some activation functions produce distributions with a higher maximum entropy than the input distribution. While it is known that the data processing inequality does no longer hold after the addition noise through binning, it should be investigated whether this is a systematic effect.\n",
    "\n",
    "It also needs to be determined which way of binning (over the global range or over the range for each weight) is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
